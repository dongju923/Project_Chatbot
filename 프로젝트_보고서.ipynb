{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "프로젝트 보고서 템플릿.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 상담사챗봇"
      ],
      "metadata": {
        "id": "y7yDJKQz7AW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프로젝트 목적\n",
        "\n",
        "* 상담사 챗봇을 통해 인건비를 줄일 수 있다.\n",
        "* 상담사가 답변하지 못하는 답변을 할 수 있다.\n",
        "* 언제 어디서는 상담을 받을 수 있어 편리하다."
      ],
      "metadata": {
        "id": "DDm7stL-7SKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프로젝트 개요\n",
        "\n",
        "* AI허브에 있는 민원 질의응답 데이터 사용(https://aihub.or.kr/aidata/30716)\n",
        "* sentencepiece와 Transformer 모델을 사용하여 챗봇을 구현\n"
      ],
      "metadata": {
        "id": "qwfe4U_M7T3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 프로젝트 내용\n",
        "\n",
        "* AI허브에 있는 데이터중 금융보험 데이터만 사용\n",
        "* json을 파싱하여 필요한 데이터만 추출\n",
        "* Transformer와 embedding 모델을 사용하여 학습"
      ],
      "metadata": {
        "id": "I2JFeBB37kTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 필요한 모듈 import\n",
        "\n",
        "* 데이터셋: https://aihub.or.kr/aidata/30716"
      ],
      "metadata": {
        "id": "vVEpG0KZ7lTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Transformer\n",
        "from torch import nn\n",
        "import torch\n",
        "import math"
      ],
      "metadata": {
        "id": "Wfbvl56J7z-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 로드 및 전처리"
      ],
      "metadata": {
        "id": "0X1tkINCOVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# json파싱후 불필요한 데이터 삭제\n",
        "dir_path = \"/home/dilab05/work_directory/adj/NLP_Project/Training/금융보험/민원(콜센터) 질의응답_금융보험_잔고 및 거래내역_Training.json\"\n",
        "with open(dir_path, 'r') as file:\n",
        "    li = []\n",
        "    data = json.load(file)\n",
        "    for i in range(len(data)):\n",
        "        del(data[i]['도메인'])\n",
        "        del(data[i]['카테고리'])\n",
        "        del(data[i]['대화셋일련번호'])\n",
        "        del(data[i]['화자'])\n",
        "        del(data[i]['문장번호'])\n",
        "        del(data[i]['고객의도'])\n",
        "        del(data[i]['상담사의도'])\n",
        "        del(data[i]['QA'])\n",
        "        del(data[i]['개체명 '])\n",
        "        del(data[i]['용어사전'])\n",
        "        del(data[i]['지식베이스'])\n",
        "        del(data[i]['상담사질문(요청)'])\n",
        "        del(data[i]['고객답변'])\n",
        "    for i in range(len(data)):    # 84031\n",
        "        if (data[i]['고객질문(요청)'] == '') and (data[i]['상담사답변'] == ''):\n",
        "            del data[i]['고객질문(요청)']\n",
        "            del data[i]['상담사답변']\n",
        "        else:\n",
        "            if data[i]['고객질문(요청)'] == '':\n",
        "                del data[i]['고객질문(요청)']\n",
        "            elif data[i]['상담사답변'] == '':\n",
        "                del data[i]['상담사답변']\n",
        "            else:\n",
        "                del data[i]['고객질문(요청)']\n",
        "                del data[i]['상담사답변']\n",
        "            li.append(data[i])"
      ],
      "metadata": {
        "id": "J5JF2KPNOdx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 고객 질문 다음 상담사 답변이 오는 것만 추출\n",
        "idxa = []\n",
        "idxq = []\n",
        "for i in range(len(li)):\n",
        "    if '고객질문(요청)' in li[i] and '상담사답변' in li[i+1]:\n",
        "        idxq.append(i)"
      ],
      "metadata": {
        "id": "VwXj_FAGOiHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매핑하기 위해 넘파이 배열 선언\n",
        "li = np.array(li)\n",
        "idxa = np.array(idxa)\n",
        "idxq = np.array(idxq)\n",
        "\n",
        "print(li)\n",
        "print(idxa)\n",
        "print(idxq)"
      ],
      "metadata": {
        "id": "PHIrTn_EOidH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 매핑\n",
        "q = li[idxq]\n",
        "a = li[idxq+1]\n",
        "\n",
        "#question = []\n",
        "#answer = []\n",
        "for i in range(len(q)):\n",
        "    question.append(q[i]['고객질문(요청)'])\n",
        "for i in range(len(q)):\n",
        "    answer.append(a[i]['상담사답변'])"
      ],
      "metadata": {
        "id": "-NufPIO6Osis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 넘파이 배열로 저장\n",
        "#np.savez_compressed('./data', question=question, answer=answer)"
      ],
      "metadata": {
        "id": "3Mc0WpE1OstI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드\n",
        "question = np.load('./data.npz')['question']\n",
        "answer = np.load('./data.npz')['answer']"
      ],
      "metadata": {
        "id": "AbVNf0HcO5Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 판다스 데이터 프레임 형식으로 저장\n",
        "train_data = pd.DataFrame([ x for x in zip(question, answer)], columns = ['Q','A'])\n",
        "train_data"
      ],
      "metadata": {
        "id": "ans6ypZyO5FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 공백제거\n",
        "q_null = train_data[train_data['Q'] == ' '].index\n",
        "train_data = train_data.drop(q_null)\n",
        "a_null = train_data[train_data['A'] == ' '].index\n",
        "train_data = train_data.drop(a_null)"
      ],
      "metadata": {
        "id": "3fEPVpCTPDem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불필요한 특수문자 제거\n",
        "questions = []\n",
        "for sentence in train_data['Q']:\n",
        "\t# 구두점에 대해서 띄어쓰기\n",
        "    # ex) 12시 땡! -> 12시 땡 !\n",
        "    sentence = re.sub(r\"([?.!,~])\", r\"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    questions.append(sentence)\n",
        "\n",
        "answers = []\n",
        "for sentence in train_data['A']:\n",
        "    sentence = re.sub(r\"([?.!,~])\", r\"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    answers.append(sentence)"
      ],
      "metadata": {
        "id": "XWmn54jePDvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문과 답변을 순서대로 txt파일로 저장\n",
        "with open('data.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(questions))\n",
        "    f.write('\\n'.join(answers))"
      ],
      "metadata": {
        "id": "x01C3CAHPD33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentencepiece를 사용하여 8000개의 vocab size를 가지고 사용자 지정 토큰 7개를 추가로 가지고 있는 sentence piece를 학습\n",
        "corpus = \"data.txt\"\n",
        "prefix = \"chatbot\"\n",
        "vocab_size = 8000\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
      ],
      "metadata": {
        "id": "-05SxWRjPEAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화 테스트\n",
        "vocab_file = \"chatbot.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "line = \"무엇을 도와드릴까요?\"\n",
        "pieces = vocab.encode_as_pieces(line)\n",
        "ids = vocab.encode_as_ids(line)\n",
        "\n",
        "\n",
        "print(line)\n",
        "print(pieces)"
      ],
      "metadata": {
        "id": "7Ln1gS4vPEIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구현\n"
      ],
      "metadata": {
        "id": "Pcu5I_HWPiW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 sentence piece를 이용하여 주어진 문장을 정수로 인코딩하는 함수를 선언. \n",
        "# 문장의 처음과 끝에는 sentence piece를 학습 시킬 때 따로 선언했던 START_TOKEN과 END_TOKEN의 index를 넣음\n",
        "MAX_LENGTH = 40\n",
        "\n",
        "START_TOKEN = [2]\n",
        "END_TOKEN = [3]\n",
        "\n",
        "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "    tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
        "        zeros1 = np.zeros(MAX_LENGTH, dtype=int)\n",
        "        zeros2 = np.zeros(MAX_LENGTH, dtype=int)\n",
        "        sentence1 = START_TOKEN + vocab.encode_as_ids(sentence1) + END_TOKEN\n",
        "        zeros1[:len(sentence1)] = sentence1[:MAX_LENGTH]\n",
        "\n",
        "        sentence2 = START_TOKEN + vocab.encode_as_ids(sentence2) + END_TOKEN\n",
        "        zeros2[:len(sentence2)] = sentence2[:MAX_LENGTH]\n",
        "\n",
        "        tokenized_inputs.append(zeros1)\n",
        "        tokenized_outputs.append(zeros2)\n",
        "    return tokenized_inputs, tokenized_outputs"
      ],
      "metadata": {
        "id": "LkNNTSrNPmMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코딩 테스트\n",
        "questions_encode, answers_encode = tokenize_and_filter(questions, answers)\n",
        "print(questions_encode[1])\n",
        "print(answers_encode[1])"
      ],
      "metadata": {
        "id": "7Jd5Vi5dPmcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch 학습을 위해 dataset, dataloader를 생성\n",
        "# 첫번째 값은 주어진 질문, 두번째 값은 디코더의 입력으로 마지막 토큰값이 제거된 대답, 마지막 값은 첫 토큰값이 제거된 결과\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        questions = np.array(questions)\n",
        "        answers = np.array(answers)\n",
        "        self.inputs = questions\n",
        "        self.dec_inputs = answers[:,:-1]\n",
        "        self.outputs = answers[:,1:]\n",
        "        self.length = len(questions)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return (self.inputs[idx], self.dec_inputs[idx], self.outputs[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "dataset = SequenceDataset(questions_encode, answers_encode)\n",
        "dataloader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "SpYBgDAUPmxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer 모델 선언(positional encoding과 Embedding이 포함)\n",
        "class TFModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TFModel, self).__init__()\n",
        "        self.transformer = Transformer(ninp, nhead, dim_feedforward=nhid, num_encoder_layers=nlayers, num_decoder_layers=nlayers,dropout=dropout)\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        self.pos_encoder_d = PositionalEncoding(ninp, dropout)\n",
        "        self.encoder_d = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.ntoken = ntoken\n",
        "\n",
        "        self.linear = nn.Linear(ninp, ntoken)\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, tgt, srcmask, tgtmask, srcpadmask, tgtpadmask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        tgt = self.encoder_d(tgt) * math.sqrt(self.ninp)\n",
        "        tgt = self.pos_encoder_d(tgt)\n",
        "\n",
        "\n",
        "        output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask, src_key_padding_mask=srcpadmask, tgt_key_padding_mask=tgtpadmask)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def gen_attention_mask(x):\n",
        "    mask = torch.eq(x, 0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "mv8PdNSFPq-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 학습 및 평가"
      ],
      "metadata": {
        "id": "Pr1OnwFA7nJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델학습\n",
        "device = torch.device(\"cuda\", index=1)\n",
        "\n",
        "lr = 1e-4\n",
        "model = TFModel(vocab_size+7, 256, 8, 512, 2, 0.1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "L5bAS2hT7zse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 20\n",
        "best_loss = 2000.0\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.load_state_dict(torch.load(\"chatbot.pth\"))\n",
        "model.train()\n",
        "for i in range(epoch):\n",
        "    batchloss = 0.0\n",
        "    progress = tqdm(dataloader)\n",
        "    for (inputs, dec_inputs, outputs) in progress:\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).to(device)\n",
        "        src_padding_mask = gen_attention_mask(inputs).to(device)\n",
        "        tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).to(device)\n",
        "        tgt_padding_mask = gen_attention_mask(dec_inputs).to(device)\n",
        "\n",
        "        result = model(inputs.to(device), dec_inputs.to(device), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask)\n",
        "        loss = criterion(result.permute(1,2,0), outputs.to(device).long())\n",
        "        progress.set_description(\"{:0.3f}\".format(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batchloss += loss\n",
        "    print(loss)\n",
        "    print(batchloss)\n",
        "    if best_loss > batchloss:\n",
        "        print(f\"Best loss: {best_loss}, Loss: {batchloss}\")\n",
        "        best_loss = batchloss\n",
        "        torch.save(model.state_dict(), \"chatbot.pth\")\n",
        "            \n",
        "    print(\"epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(dataloader))"
      ],
      "metadata": {
        "id": "-1cIC4AsQA8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "def evaluate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    input = torch.tensor([START_TOKEN + vocab.encode_as_ids(sentence) + END_TOKEN]).to(device)\n",
        "    output = torch.tensor([START_TOKEN]).to(device)\n",
        "\n",
        "    # 디코더의 예측 시작\n",
        "    model.eval()\n",
        "    for i in range(MAX_LENGTH):\n",
        "        src_mask = model.generate_square_subsequent_mask(input.shape[1]).to(device)\n",
        "        tgt_mask = model.generate_square_subsequent_mask(output.shape[1]).to(device)\n",
        "\n",
        "        src_padding_mask = gen_attention_mask(input).to(device)\n",
        "        tgt_padding_mask = gen_attention_mask(output).to(device)\n",
        "\n",
        "        predictions = model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n",
        "        # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = torch.LongTensor(torch.argmax(predictions.cpu(), axis=-1))\n",
        "\n",
        "\n",
        "        # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "        if torch.equal(predicted_id[0][0], torch.tensor(END_TOKEN[0])):\n",
        "            break\n",
        "\n",
        "        # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "        # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "        output = torch.cat([output, predicted_id.to(device)], axis=1)\n",
        "\n",
        "    return torch.squeeze(output, axis=0).cpu().numpy()\n",
        "\n",
        "def predict(sentence):\n",
        "    prediction = evaluate(sentence)\n",
        "    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size+7])))\n",
        "\n",
        "    print('Input: {}'.format(sentence))\n",
        "    print('Output: {}'.format(predicted_sentence))\n",
        "\n",
        "    return predicted_sentence"
      ],
      "metadata": {
        "id": "oTRYCym7P7Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과 및 응용"
      ],
      "metadata": {
        "id": "hVhayYzY7rlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHJTHBTA6_rt"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"chatbot.pth\"))\n",
        "result = predict(\"보험적용이 되나요?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 참고 문헌\n",
        "\n",
        "* sentencepiece: https://wikidocs.net/86657\n"
      ],
      "metadata": {
        "id": "S1w--GRk7cYn"
      }
    }
  ]
}